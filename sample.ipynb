{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p 'data/paul_graham/'\n",
    "# !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup prompts - specific to StableLM\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df480361dac448aea3a6e814b11d5924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\SLY\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\llama_index\\embeddings\\utils.py:48\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[1;34m(embed_model)\u001b[0m\n\u001b[0;32m     47\u001b[0m     embed_model \u001b[39m=\u001b[39m OpenAIEmbedding()\n\u001b[1;32m---> 48\u001b[0m     validate_openai_api_key(embed_model\u001b[39m.\u001b[39;49mapi_key)\n\u001b[0;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\SLY\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\llama_index\\llms\\openai_utils.py:371\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m openai_api_key:\n\u001b[1;32m--> 371\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[1;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SLY\\Documents\\ChatNU\\sample.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SLY/Documents/ChatNU/sample.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m service_context \u001b[39m=\u001b[39m ServiceContext\u001b[39m.\u001b[39;49mfrom_defaults(chunk_size\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m, llm\u001b[39m=\u001b[39;49mllm)\n",
      "File \u001b[1;32mc:\\Users\\SLY\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\llama_index\\service_context.py:180\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[1;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, text_splitter, transformations, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[0;32m    175\u001b[0m         llm_predictor\u001b[39m.\u001b[39mquery_wrapper_prompt \u001b[39m=\u001b[39m query_wrapper_prompt\n\u001b[0;32m    177\u001b[0m \u001b[39m# NOTE: the embed_model isn't used in all indices\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[39m# NOTE: embed model should be a transformation, but the way the service\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39m# context works, we can't put in there yet.\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m embed_model \u001b[39m=\u001b[39m resolve_embed_model(embed_model)\n\u001b[0;32m    181\u001b[0m embed_model\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager\n\u001b[0;32m    183\u001b[0m prompt_helper \u001b[39m=\u001b[39m prompt_helper \u001b[39mor\u001b[39;00m _get_default_prompt_helper(\n\u001b[0;32m    184\u001b[0m     llm_metadata\u001b[39m=\u001b[39mllm_predictor\u001b[39m.\u001b[39mmetadata,\n\u001b[0;32m    185\u001b[0m     context_window\u001b[39m=\u001b[39mcontext_window,\n\u001b[0;32m    186\u001b[0m     num_output\u001b[39m=\u001b[39mnum_output,\n\u001b[0;32m    187\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\SLY\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\llama_index\\embeddings\\utils.py:50\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[1;34m(embed_model)\u001b[0m\n\u001b[0;32m     48\u001b[0m         validate_openai_api_key(embed_model\u001b[39m.\u001b[39mapi_key)\n\u001b[0;32m     49\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 50\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     51\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m******\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCould not load OpenAI embedding model. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mOriginal error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m!s}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mConsider using embed_model=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mVisit our documentation for more embedding options: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mhttps://docs.llamaindex.ai/en/stable/module_guides/models/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39membeddings.html#modules\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m******\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m         )\n\u001b[0;32m     63\u001b[0m \u001b[39m# for image embeddings\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mif\u001b[39;00m embed_model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: \n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm, embed_model=\"local\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
